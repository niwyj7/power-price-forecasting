import joblib
import sys
sys.path.append("/data3/XXXXXX/")
from EnergyDataLoader.energydataloader import EnergyDataLoader
# import plotly.express as px
ed = EnergyDataLoader('location')
import numpy as np
import os
import pandas as pd
from sklearn.linear_model import Lasso
from EnergySQL.energysql import EnergySQL
from datetime import datetime, timedelta
from lightgbm import LGBMRegressor
import plotly.express as px


from sklearn.preprocessing import StandardScaler


### Data Preparing
def select_features(features=[], NN=0,
                    startdate='20250101', enddate='20250131'):
    start_dt = pd.to_datetime(str(startdate), format='%Y%m%d') 
    
    df_train = esql.select(features, start=startdate, end=enddate, NN=NN)
 
    df_train['latitude'] = pd.to_numeric(df_train.index.get_level_values('latlon').str.extract(r'(\d+\.\d+)N', expand=False), errors='coerce')
    df_train['longitude'] = pd.to_numeric(df_train.index.get_level_values('latlon').str.extract(r'(\d+\.\d+)E', expand=False), errors='coerce')
    df_train.reset_index(inplace=True)
    df_train.set_index('datetime', inplace=True)
    df_train.drop(columns=['latlon', 'T'], inplace=True)
    print (df_train)
    return df_train

def get_node_idx(df_train):
    unique_latitude = np.sort(df_train['latitude'].unique())
    unique_longitude = np.sort(df_train['longitude'].unique())
    df_train['latitude_rank'] = df_train['latitude'].map(dict(zip(unique_latitude, range(len(unique_latitude)))))
    df_train['longitude_rank'] = df_train['longitude'].map(dict(zip(unique_longitude, range(len(unique_longitude)))))
    sr_pos = df_train['latitude'] * df_train['longitude']
    unique_pos = np.sort(sr_pos.unique())
    df_train['idx'] = sr_pos.map(dict(zip(unique_pos, range(len(unique_pos)))))
    return df_train

def feature_engineering00(df_train):
    df_train['wind_speed_100m_log'] = np.log(1 + df_train['win100_spd'])
    df_train['wind_cube'] = df_train['win100_spd'] ** 3
    # df_train['wind_gusts_10m_log'] = np.log(1 + df_train['wind_gusts_10m'])
    # df_train[['wind_gusts_10m_diff', 'wind_speed_100m_diff']] = df_train[['wind_gusts_10m', 'wind_speed_100m', 'idx']].groupby(by='idx', sort=False).transform(lambda sr: sr.diff(1))
    # df_train[['wind_gusts_10m_diff_12', 'wind_speed_100m_diff_12']] = df_train[['wind_gusts_10m', 'win100_spd', 'idx']].groupby(by='idx', sort=False).transform(lambda sr: sr.diff(4))
    # df_train['la_speed_100m'] = df_train['wind_speed_100m']* np.cos(np.deg2rad(df_train['wind_direction_100m']))
    # df_train['lo_speed_100m'] = df_train['wind_speed_100m']* np.sin(np.deg2rad(df_train['wind_direction_100m']))
    df_train['la_speed_cube']= df_train['u100'] ** 3
    df_train['lo_speed_cube']= df_train['v100'] ** 3
    df_train['temperature_ave'] = df_train['t2'].resample('D').transform('mean')
    df_train['hour']= pd.to_datetime(df_train.index).hour + pd.to_datetime(df_train.index).minute / 60
    df_train['dayofweek'] = pd.to_datetime(df_train.index).dayofweek
    # 数据标准化：对部分连续特征做标准化（去均值 / 标准差）
    # 这里只对已经存在的部分特征做示例性标准化，可按需增减

    existing_cols = df_train.columns

    scaler = StandardScaler()
    df_train[existing_cols] = scaler.fit_transform(df_train[existing_cols])
    df_train.dropna(inplace=True)
    print(df_train)
    # 将小时频率插值到 15 分钟频率（保持每个 time × latlon 的所有特征一起做插值）
    df_train = (
        df_train
        .groupby(['latitude_rank', 'longitude_rank'])        # 每个网格点单独插值
        .apply(
            lambda g: g.resample('15min').interpolate(method='cubic')  # 时间维度上做 cubic 插值
        )
        # .drop(columns=['latitude_rank', 'longitude_rank'])  # groupby 会把分组键提到 index 里，插值后再删掉
    )
    df_train['ssrd_diff'] = df_train['ssrd'] - df_train['ssrd'].shift(8).ffill().bfill()
    df_train['wind_diff'] = df_train['win100_spd'] - df_train['win100_spd'].shift(8).ffill().bfill()
    return df_train

def feature_engineering(df_train):
    df_train['wind_speed_100m_log'] = np.log1p(df_train['win100_spd'])
    df_train['wind_cube'] = df_train['win100_spd'] ** 3
    df_train['la_speed_cube'] = df_train['u100'] ** 3
    df_train['lo_speed_cube'] = df_train['v100'] ** 3

    df_train['hour'] = df_train.index.hour + df_train.index.minute / 60
    df_train['dayofweek'] = df_train.index.dayofweek


    # def reindex_and_interpolate(g):
    #     start_date = g.index.min().normalize()
    #     end_date = g.index.max().normalize() + pd.Timedelta(hours=23, minutes=45)
        
    #     full_grid = pd.date_range(start=start_date, end=end_date, freq='15min')
        
    #     return g.reindex(full_grid).interpolate(method='cubic').ffill().bfill()
    start_date = df_train.index.min().normalize()
    end_date = df_train.index.max().normalize() + pd.Timedelta(hours=23, minutes=45)
    full_grid = pd.date_range(start=start_date, end=end_date, freq='15min')

    def reindex_group(g):
        # We only reindex to the master grid
        return g.reindex(full_grid).interpolate(method='cubic').ffill().bfill()
    # df_train = (
    #     df_train
    #     .groupby(['latitude_rank', 'longitude_rank'], group_keys=False)
    #     .apply(reindex_and_interpolate)
    # )
    df_train = (
        df_train
        .groupby(['latitude_rank', 'longitude_rank'], group_keys=False)
        .apply(reindex_group)
    )
    
    df_train['temperature_ave'] = df_train.groupby(['latitude_rank', 'longitude_rank'])['t2'].transform(
        lambda x: x.resample('D').mean().reindex(x.index, method='ffill')
    )

    cols_to_scale = df_train.columns
    scaler = StandardScaler()
    df_train[cols_to_scale] = scaler.fit_transform(df_train[cols_to_scale])

    print(f"Final shape: {df_train.shape}")
    print(df_train)
    # df_train=df_train.dropna()  
    return df_train
def str_date_delta(date='20250201', date_delta=-30):
    return (pd.Timestamp(date) + pd.Timedelta(days=date_delta)).strftime('%Y%m%d')

def prepare_data(raw_features=[], features=[], NN=0, 
                 startdate='20250101', enddate='20250131', train=True):
    df_feature = select_features(features=raw_features, NN=NN,
                                 startdate=str_date_delta(startdate, -1), enddate=enddate)  # minus one day for feature engineering
    df_feature = get_node_idx(df_feature)
    df_feature = feature_engineering(df_feature)
    df_feature_ = df_feature[features + ['idx']].set_index('idx', append=True).sort_index()

    if train:
        ed = EnergyDataLoader("shaanxi")
        y_ = ed.pull(['da'], start=startdate, end=enddate)
        y_ = y_['da'].apply(lambda x: -400 if x == 0 else x)
        print(y_)
        # y_ = y.resample('h', label='left', closed='left').mean()
        # y_ = np.sqrt(y_)  # 对目标值做平方根变换以稳定方差
        # y_= y_.lambda x: np.log(x.clip(lower=1))  # 对目标值做对数变换以稳定方差
        # y_= np.log(y_.clip(lower=1))  # 对目标值做对数变换以稳定方差
        # 将 0 替换为 -400（逐元素比较，而不是对整列做布尔判断）
        y_ = y_.where(y_ != 0, -400)
    else:
        y_ = pd.DataFrame(
            np.nan,
            index=pd.date_range(start=startdate, end=str_date_delta(enddate, 1), freq='15min', inclusive='left'),
            columns=['da']
        )
    start_datetime, end_datetime = f'{startdate} 00:00:00', f'{enddate} 23:45:00'
    return df_feature_.loc[start_datetime: end_datetime], y_.loc[start_datetime: end_datetime]


### Data Loader
class RollingDataset(Dataset):    
    def __init__(self, feature_df, target_df, window_size, stride=1, target_with_time=False):
        self.feature_df = feature_df
        self.target_df = target_df
        self.window_size = window_size
        self.stride = stride
        self.target_with_time = target_with_time
        
        self.time_index = target_df.index
        self.n_nodes = len(feature_df.loc[self.time_index[0]])
        self.n_features = feature_df.shape[1]
        self.n_samples = (len(self.time_index) - window_size) // stride + 1
        
    def __len__(self):
        return self.n_samples
    
    def __getitem__(self, idx):
        start_idx = idx * self.stride
        end_idx = start_idx + self.window_size        
        time_window = self.time_index[start_idx:end_idx]
        # 提取特征序列 [l, n, d]
        feature_list = [self.feature_df.loc[t].values for t in time_window]
        X = torch.FloatTensor(np.stack(feature_list, axis=0))
        # 提取目标序列 [l, 1]
        if not self.target_with_time:
            y = torch.FloatTensor(self.target_df.loc[time_window].values)
        else:
            y = self.target_df.loc[time_window]
        return X, y  
        
