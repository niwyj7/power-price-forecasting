### Data Loader
class RollingDataset(Dataset):    
    def __init__(self, feature_df, target_df, window_size, stride=1, target_with_time=False):
        self.feature_df = feature_df
        self.target_df = target_df
        self.window_size = window_size
        self.stride = stride
        self.target_with_time = target_with_time
        
        self.time_index = target_df.index
        self.n_nodes = len(feature_df.loc[self.time_index[0]])
        self.n_features = feature_df.shape[1]
        self.n_samples = (len(self.time_index) - window_size) // stride + 1
        
    def __len__(self):
        return self.n_samples
    
    def __getitem__(self, idx):
        start_idx = idx * self.stride
        end_idx = start_idx + self.window_size        
        time_window = self.time_index[start_idx:end_idx]
        # 提取特征序列 [l, n, d]
        feature_list = [self.feature_df.loc[t].values for t in time_window]
        X = torch.FloatTensor(np.stack(feature_list, axis=0))
        # 提取目标序列 [l, 1]
        if not self.target_with_time:
            y = torch.FloatTensor(self.target_df.loc[time_window].values)
        else:
            y = self.target_df.loc[time_window]
        return X, y

class RollingDataLoader:
    def __init__(self, feature_df, target_df, window_size, stride=1, shuffle=True, train=True):
        self.dataset = RollingDataset(feature_df, target_df, window_size, stride,
                                      target_with_time=False if train else True)
        self.shuffle = shuffle
        self.train = train
        self.indices = list(range(len(self.dataset)))
        
    def __len__(self):
        return len(self.dataset)
    
    def __iter__(self):
        if self.shuffle:
            np.random.shuffle(self.indices)
        for idx in self.indices:
            yield self.dataset[idx]  # X: [l, n, d], y: [l, 1]

### Model
class MovingAvg(nn.Module):
    """
        Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super().__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class SeriesDecomp(nn.Module):
    """
        Series decomposition block
    """
    def __init__(self, kernel_size):
        super().__init__()
        self.moving_avg = MovingAvg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class DLinear(nn.Module):
    def __init__(self, seq_len, pred_len, input_dim, individual=False):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len

        # Decompsition Kernel Size
        kernel_size = 25
        self.decompsition = SeriesDecomp(kernel_size)
        self.individual = individual
        self.channels = input_dim

        if self.individual:
            self.Linear_Seasonal = nn.ModuleList()
            self.Linear_Trend = nn.ModuleList()
            self.Linear_Decoder = nn.ModuleList()
            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Seasonal[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))
                self.Linear_Decoder.append(nn.Linear(self.seq_len,self.pred_len))
        else:
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Decoder = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Seasonal.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))
            self.Linear_Trend.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))
    def reset_parameters(self):
        pass
    def forward(self, x):
        # x: [Batch, seq_len, Channel]
        seasonal_init, trend_init = self.decompsition(x)
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # -> [Batch, pred_len, Channel]

class DLNet(nn.Module):
    def __init__(self, input_dim, seq_len, nodes):
        super().__init__()
        self.fcn = nn.Sequential(
            nn.Linear(input_dim, 2 * input_dim, bias=True),
            nn.Dropout(0.2),
            nn.ReLU(),
            nn.Linear(2 * input_dim, input_dim, bias=True)
        )
        self.dlinear = DLinear(seq_len, seq_len, input_dim=input_dim)
        self.dlinear_norm = nn.LayerNorm(input_dim)
        self.fcn_feature = nn.Linear(input_dim, 1, bias=True)
        self.fcn_graph = nn.Linear(nodes, 1, bias=False)
        self.reset_parameters()
    def reset_parameters(self):
        for i, module in enumerate(self.fcn):
            if isinstance(module, nn.Linear):
                nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
        self.dlinear.reset_parameters()
        nn.init.kaiming_uniform_(self.fcn_feature.weight, mode='fan_in', nonlinearity='relu')
        if self.fcn_feature.bias is not None:
            nn.init.constant_(self.fcn_feature.bias, 0)
        nn.init.xavier_uniform_(self.fcn_graph.weight)

    def forward(self, x):
        x = self.fcn(x)
        x = F.relu(x)
        x = self.dlinear(x.permute(1, 0, 2))  # after permute, x: (n, l, nfeats)
        x = self.dlinear_norm(x)
        x = F.relu(x)
        x = self.fcn_feature(x.permute(1, 0, 2))  # after permute, x: (l, n, nfeats)
        x = F.relu(x)
        y = self.fcn_graph(x.squeeze(-1))  # after squeeze, x: (l, n)
        return y

### Training
def set_seeds(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)

def train_one_epoch(model, loader, optimizer, criterion, device, l1_lambda = 0.001):
    model.train()
    total_loss = 0.0
    total_samples = 0
    for X, y in loader:
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        y_ = model(X)
        loss = criterion(y_, y) + l1_lambda * torch.abs(model.fcn_graph.weight).sum()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * y.size(0)
        total_samples += y.size(0)
    avg_loss = total_loss / total_samples
    return avg_loss

def train_model(raw_features, features,
                enddate='20250201', train_lookback_dates=30,
                data_loader_params={}, model_params={}):
    set_seeds(model_params.get('seed', 22))
    # data preparing
    startdate = str_date_delta(enddate, -train_lookback_dates)
    feature_df, target_df = prepare_data(raw_features, features, NN=0,
                                         startdate=startdate, enddate=enddate, train=True)
    train_loader = RollingDataLoader(
        feature_df, target_df,
        window_size=data_loader_params.get('window_size', 24),
        stride=data_loader_params.get('stride', 1),
        shuffle=data_loader_params.get('shuffle', True),
        train=True
    )
    
    # model initializing
    device = model_params.get('device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
    model = DLNet(input_dim=train_loader.dataset.n_features,
                  seq_len=train_loader.dataset.window_size,
                  nodes=train_loader.dataset.n_nodes)
    model = model.to(device)
    criterion = nn.MSELoss(reduction='mean')
    optimizer = torch.optim.Adam(model.parameters(),
                                 lr=model_params.get('lr', 0.003), weight_decay=model_params.get('weight_decay', 0))
    l1_lambda = model_params.get('l1_lambda', 0.8)
    # training
    if model_params.get('verbose', 1):
        print(f'---Start Training from {startdate} to {enddate}---')
    num_epochs = model_params.get('num_epochs', 20)
    for epoch in range(1, num_epochs+1):
        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, l1_lambda=l1_lambda)
        if model_params.get('verbose', 1):
            print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.2f}')
    return model
    
